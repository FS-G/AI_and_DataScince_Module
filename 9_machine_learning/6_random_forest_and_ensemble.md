
# Random Forest and Ensemble Methods

## Table of Contents
1. [Introduction to Ensemble Learning](#introduction-to-ensemble-learning)
2. [Bagging (Bootstrap Aggregating)](#bagging-bootstrap-aggregating)
3. [Boosting Methods](#boosting-methods)
4. [Random Forest](#random-forest)
5. [Model Comparison and Selection](#model-comparison-and-selection)
6. [Hyperparameter Tuning](#hyperparameter-tuning)
7. [Practical Applications](#practical-applications)
8. [Hands-on Exercises](#hands-on-exercises)

---

## Introduction to Ensemble Learning

### What is Ensemble Learning?

**The Problem**: Individual machine learning models can make mistakes due to:
- **Bias**: Model assumptions that don't match reality
- **Variance**: Model sensitivity to small changes in training data
- **Noise**: Random errors in the data

**The Solution**: Ensemble Learning - Combine multiple models to create a more robust predictor!

### The Wisdom of Crowds Analogy

Think of ensemble learning like asking multiple experts for advice:
- **Single Expert**: Might have blind spots or biases
- **Multiple Experts**: Different perspectives compensate for individual weaknesses
- **Combined Decision**: Usually more accurate than any single expert

### Types of Ensemble Methods

| Method | How Models Train | Key Characteristic | Best For |
|--------|------------------|-------------------|----------|
| **Bagging** | Parallel | Reduces variance | Stable base learners |
| **Boosting** | Sequential | Reduces bias | Weak learners |
| **Stacking** | Meta-learning | Learns optimal combination | Complex relationships |

### Why Ensembles Work

1. **Error Reduction**: Different models make different errors
2. **Robustness**: Less sensitive to outliers and noise
3. **Generalization**: Better performance on unseen data
4. **Confidence**: More reliable predictions

## Bagging (Bootstrap Aggregating)

### Core Concept

**Bagging** = **B**ootstrap **Agg**regat**ing**

The idea is simple but powerful:
1. **Bootstrap**: Create multiple datasets by sampling with replacement
2. **Aggregating**: Combine predictions from all models

### Why Bootstrap Sampling?

**Problem**: Limited training data
**Solution**: Create "virtual" datasets through resampling

**Key Insight**: Each bootstrap sample is different, so each model learns different patterns!

### Bootstrap Sampling Example

```python
# Original dataset
original_data = [1, 2, 3, 4, 5]

# Bootstrap samples (with replacement)
sample_1 = [1, 1, 3, 4, 5]  # 1 appears twice, 2 missing
sample_2 = [2, 3, 3, 4, 4]  # 3,4 appear twice, 1,5 missing  
sample_3 = [1, 2, 2, 5, 5]  # 2,5 appear twice, 3,4 missing
```

**Notice**: Each sample has the same size but different composition!

### Bagging Algorithm Step-by-Step

#### Our Example Dataset
| Row | Feature A | Feature B | Target |
|-----|-----------|-----------|---------|
| 1   | 2         | 3         | 1       |
| 2   | 5         | 4         | 0       |
| 3   | 3         | 7         | 1       |
| 4   | 6         | 8         | 0       |
| 5   | 9         | 1         | 1       |

#### Step 1: Create Bootstrap Samples
- **Sample 1**: Rows [2, 3, 2, 4, 5] → Train Model 1
- **Sample 2**: Rows [1, 5, 4, 1, 3] → Train Model 2  
- **Sample 3**: Rows [5, 3, 4, 5, 2] → Train Model 3

#### Step 2: Train Individual Models
Each model learns from its unique bootstrap sample

#### Step 3: Combine Predictions
- **Classification**: Majority vote (most common prediction)
- **Regression**: Average of all predictions

### Bagging Benefits

✅ **Reduces Variance**: Smooths out individual model errors  
✅ **Parallel Training**: Models train independently  
✅ **Robust**: Less sensitive to outliers  
✅ **Simple**: Easy to implement and understand

### Bagging Implementation

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Create sample data
X, y = make_classification(n_samples=1000, n_features=10, 
                          n_classes=2, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Bagging with decision trees
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=5),
    n_estimators=50,        # Number of models
    max_samples=0.8,        # Fraction of samples for each bootstrap
    max_features=0.8,       # Fraction of features for each model
    bootstrap=True,         # Use bootstrap sampling
    random_state=42
)

# Train the ensemble
bagging.fit(X_train, y_train)

# Make predictions
predictions = bagging.predict(X_test)
accuracy = accuracy_score(y_test, predictions)

print(f"Bagging Accuracy: {accuracy:.3f}")
print("\nClassification Report:")
print(classification_report(y_test, predictions))

# Get prediction probabilities
probabilities = bagging.predict_proba(X_test)
print(f"\nPrediction Probabilities Shape: {probabilities.shape}")
```

### Understanding Bagging Parameters

| Parameter | Description | Typical Value | Impact |
|-----------|-------------|---------------|---------|
| `n_estimators` | Number of models | 50-200 | More models = better performance |
| `max_samples` | Bootstrap sample size | 0.8-1.0 | Smaller = more diversity |
| `max_features` | Features per model | 0.8-1.0 | Smaller = more diversity |
| `bootstrap` | Use sampling with replacement | True | Essential for bagging |

## Boosting Methods

### Core Concept

**Boosting** = Train models **sequentially**, where each new model learns from the mistakes of previous models.

**Key Insight**: Instead of training models independently, we make each model focus on the samples that previous models got wrong!

### How Boosting Works

1. **Train Model 1** on original data
2. **Identify Mistakes** - Find samples Model 1 predicted incorrectly  
3. **Increase Weights** - Give more importance to these difficult samples
4. **Train Model 2** - Focus more on the weighted (difficult) samples
5. **Repeat** - Continue until desired number of models
6. **Combine** - Weighted average of all model predictions

### Boosting vs Bagging

| Aspect | Bagging | Boosting |
|--------|---------|----------|
| **Training** | Parallel | Sequential |
| **Focus** | All samples equally | Mistakes from previous models |
| **Bias/Variance** | Reduces variance | Reduces bias |
| **Overfitting** | Less prone | More prone |
| **Speed** | Faster (parallel) | Slower (sequential) |

### Boosting Example Walkthrough

**Round 1:**
- Model 1 predictions: [✓, ✗, ✓, ✓, ✗]
- Mistakes on samples 2 and 5
- Increase weights for samples 2 and 5

**Round 2:**
- Model 2 focuses more on samples 2 and 5
- Model 2 predictions: [✗, ✓, ✗, ✓, ✓]
- Different mistakes this time!

**Final Prediction:**
- Weighted combination of Model 1 + Model 2
- Usually more accurate than either alone

### Types of Boosting Algorithms

#### 1. AdaBoost (Adaptive Boosting)
- **Adaptive**: Adjusts weights based on model performance
- **Weak Learners**: Uses simple models (often decision stumps)
- **Weight Update**: Increases weights for misclassified samples

#### 2. Gradient Boosting
- **Gradient**: Uses gradient descent to minimize loss
- **Residuals**: Each model predicts the errors of previous models
- **Flexible**: Can use any differentiable loss function

#### 3. XGBoost (Extreme Gradient Boosting)
- **Optimized**: Highly optimized implementation
- **Regularization**: Built-in regularization to prevent overfitting
- **Speed**: Much faster than traditional gradient boosting

### Boosting Implementation Examples

```python
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Create sample data
X, y = make_classification(n_samples=1000, n_features=10, 
                          n_classes=2, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# AdaBoost Example
ada_boost = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),  # Decision stumps
    n_estimators=50,
    learning_rate=1.0,  # Weight given to each model
    algorithm='SAMME',  # Multi-class boosting algorithm
    random_state=42
)

ada_boost.fit(X_train, y_train)
ada_predictions = ada_boost.predict(X_test)
ada_accuracy = accuracy_score(y_test, ada_predictions)

print(f"AdaBoost Accuracy: {ada_accuracy:.3f}")

# Gradient Boosting Example  
gb_boost = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,  # Lower learning rate = more conservative
    max_depth=3,        # Deeper trees
    subsample=0.8,      # Use 80% of samples for each tree
    random_state=42
)

gb_boost.fit(X_train, y_train)
gb_predictions = gb_boost.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_predictions)

print(f"Gradient Boosting Accuracy: {gb_accuracy:.3f}")

# Compare boosting methods
print(f"\nAdaBoost Accuracy: {ada_accuracy:.3f}")
print(f"Gradient Boosting Accuracy: {gb_accuracy:.3f}")
```

### Understanding Boosting Parameters

| Parameter | AdaBoost | Gradient Boosting | Impact |
|-----------|----------|-------------------|---------|
| `learning_rate` | Weight per model | Shrinkage factor | Lower = more conservative |
| `n_estimators` | Number of models | Number of trees | More = better performance |
| `max_depth` | Base estimator depth | Tree depth | Deeper = more complex |
| `subsample` | Not available | Sample fraction | < 1.0 = stochastic boosting |







## Random Forest

### What is Random Forest?

**Random Forest** = **Bagging** + **Decision Trees** + **Extra Randomness**

It's the superstar of ensemble methods because it combines the best of both worlds:
- **Bagging**: Reduces variance through bootstrap sampling
- **Decision Trees**: Flexible, non-parametric models
- **Random Features**: Additional randomness for better generalization

### The "Forest" Analogy

Think of Random Forest like a real forest:
- **Individual Trees**: Each decision tree is like one tree in the forest
- **Different Views**: Each tree sees the data from a slightly different angle
- **Collective Wisdom**: The forest (ensemble) makes better decisions than any single tree
- **Biodiversity**: The randomness ensures trees are diverse, not identical

### Two Types of Randomness

#### 1. Random Sampling (Bootstrap)
- Each tree trains on a different bootstrap sample
- Some samples appear multiple times, others not at all
- Creates diversity in training data

#### 2. Random Features (Feature Bagging)
- At each split, only consider a random subset of features
- Prevents any single feature from dominating
- Encourages different trees to use different features

### Random Forest Algorithm

```python
def random_forest_algorithm():
    for each_tree in range(n_estimators):
        # Step 1: Bootstrap sampling
        bootstrap_sample = sample_with_replacement(training_data)
        
        # Step 2: Grow tree with random features
        tree = grow_decision_tree(bootstrap_sample, random_features=True)
        
        # Step 3: Add to forest
        forest.append(tree)
    
    # Step 4: Make predictions
    def predict(new_sample):
        predictions = []
        for tree in forest:
            predictions.append(tree.predict(new_sample))
        
        # Classification: majority vote
        # Regression: average
        return combine_predictions(predictions)
```

### Why Random Forest Works So Well

#### ✅ **Reduces Overfitting**
- Individual trees might overfit
- Averaging across many trees reduces this problem
- Bootstrap sampling provides natural regularization

#### ✅ **Handles High-Dimensional Data**
- Random feature selection prevents curse of dimensionality
- Works well even with many features relative to samples

#### ✅ **Robust to Outliers**
- Bootstrap sampling dilutes outlier influence
- Majority voting reduces impact of extreme predictions

#### ✅ **Provides Feature Importance**
- Shows which features are most useful for predictions
- Helps with feature selection and understanding

#### ✅ **Handles Missing Values**
- Can work with incomplete data
- No need for extensive preprocessing

#### ✅ **Fast and Scalable**
- Parallel training of trees
- Efficient prediction process

### Random Forest Implementation

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
import pandas as pd
import numpy as np

# Classification Example
X_class, y_class = make_classification(n_samples=1000, n_features=10, 
                                     n_classes=2, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(
    X_class, y_class, test_size=0.3, random_state=42
)

# Create Random Forest Classifier
rf_classifier = RandomForestClassifier(
    n_estimators=100,        # Number of trees
    max_depth=10,            # Maximum depth of each tree
    min_samples_split=5,     # Minimum samples to split a node
    min_samples_leaf=2,      # Minimum samples in a leaf
    max_features='sqrt',     # Number of features to consider at each split
    bootstrap=True,          # Use bootstrap sampling
    random_state=42,
    n_jobs=-1               # Use all CPU cores
)

# Train the model
rf_classifier.fit(X_train, y_train)

# Make predictions
predictions = rf_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)

print(f"Random Forest Classification Accuracy: {accuracy:.3f}")

# Regression Example
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, 
                              noise=0.1, random_state=42)

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

rf_regressor = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)

rf_regressor.fit(X_train_reg, y_train_reg)
reg_predictions = rf_regressor.predict(X_test_reg)
mse = mean_squared_error(y_test_reg, reg_predictions)

print(f"Random Forest Regression MSE: {mse:.3f}")
```

### Understanding Random Forest Parameters

| Parameter | Description | Typical Value | Impact |
|-----------|-------------|---------------|---------|
| `n_estimators` | Number of trees | 100-1000 | More trees = better performance |
| `max_depth` | Maximum tree depth | 10-20 or None | Deeper = more complex |
| `min_samples_split` | Min samples to split | 2-10 | Higher = less overfitting |
| `min_samples_leaf` | Min samples in leaf | 1-5 | Higher = smoother predictions |
| `max_features` | Features per split | 'sqrt', 'log2', or number | Lower = more randomness |
| `bootstrap` | Use bootstrap sampling | True | Essential for bagging |
| `n_jobs` | Parallel processing | -1 (all cores) | Faster training |

### Feature Importance Analysis

One of Random Forest's greatest strengths is its ability to identify important features:

```python
import matplotlib.pyplot as plt
import pandas as pd

# Get feature importance
importance = rf_classifier.feature_importances_
feature_names = [f'Feature_{i}' for i in range(X_class.shape[1])]

# Create importance dataframe
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importance
}).sort_values('importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(importance_df['feature'], importance_df['importance'])
plt.xlabel('Feature Importance')
plt.title('Random Forest - Feature Importance')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("Top 5 Most Important Features:")
print(importance_df.head())

# Feature importance interpretation
print("\nFeature Importance Interpretation:")
print("- Higher values = more important for predictions")
print("- Values sum to 1.0 across all features")
print("- Can be used for feature selection")
```

### Out-of-Bag (OOB) Score

Random Forest provides a built-in validation method:

```python
# Enable OOB scoring
rf_oob = RandomForestClassifier(
    n_estimators=100,
    oob_score=True,  # Enable out-of-bag scoring
    random_state=42
)

rf_oob.fit(X_train, y_train)

print(f"Out-of-Bag Score: {rf_oob.oob_score_:.3f}")
print("OOB Score is similar to cross-validation but faster!")
```

## Model Comparison and Selection

### Comprehensive Model Comparison

Let's compare all ensemble methods on the same dataset:

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (BaggingClassifier, AdaBoostClassifier, 
                            GradientBoostingClassifier, RandomForestClassifier)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import time

# Prepare data
X, y = make_classification(n_samples=1000, n_features=10, 
                          n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Define models
models = {
    'Single Decision Tree': DecisionTreeClassifier(random_state=42),
    'Bagging': BaggingClassifier(n_estimators=50, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42)
}

# Train and evaluate models
results = []

for name, model in models.items():
    # Time the training
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    # Make predictions
    predictions = model.predict(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    f1 = f1_score(y_test, predictions)
    
    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'Training Time (s)': training_time
    })

# Display results
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Accuracy', ascending=False)

print("Model Comparison Results:")
print(results_df.round(3))
```

### When to Use Each Method?

| Method | Best For | Pros | Cons | Use When |
|--------|----------|------|------|----------|
| **Single Decision Tree** | Small datasets, interpretability | Fast, interpretable | Prone to overfitting | Need explanations, small data |
| **Bagging** | Stable base learners | Reduces variance, parallel | Doesn't reduce bias | High variance problems |
| **AdaBoost** | Weak learners | Reduces bias, adaptive | Sensitive to noise | Need high accuracy, clean data |
| **Gradient Boosting** | High accuracy needed | Very accurate, flexible | Slow, prone to overfitting | Competition, complex patterns |
| **Random Forest** | General purpose | Robust, fast, feature importance | Less interpretable | Most practical applications |

### Decision Framework

```python
def choose_ensemble_method(data_size, interpretability_needed, accuracy_needed, 
                          training_time_available, data_quality):
    """
    Simple decision framework for choosing ensemble methods
    """
    if interpretability_needed == "High":
        return "Single Decision Tree"
    elif data_size < 1000:
        return "Single Decision Tree or Bagging"
    elif accuracy_needed == "Very High" and training_time_available == "High":
        return "Gradient Boosting or XGBoost"
    elif data_quality == "Noisy":
        return "Random Forest"
    else:
        return "Random Forest"  # Default choice

# Example usage
recommendation = choose_ensemble_method(
    data_size=5000,
    interpretability_needed="Low", 
    accuracy_needed="High",
    training_time_available="Medium",
    data_quality="Clean"
)
print(f"Recommended method: {recommendation}")
```

## Hyperparameter Tuning

### Grid Search for Random Forest

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import make_scorer

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', 0.8]
}

# Grid search with cross-validation
rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Fit grid search
rf_grid.fit(X_train, y_train)

# Best parameters and score
print("Best Parameters:", rf_grid.best_params_)
print("Best Cross-Validation Score:", rf_grid.best_score_)

# Test with best model
best_rf = rf_grid.best_estimator_
test_accuracy = accuracy_score(y_test, best_rf.predict(X_test))
print("Test Accuracy with Best Model:", test_accuracy)
```

### Randomized Search (Faster Alternative)

```python
from scipy.stats import randint, uniform

# Define parameter distributions
param_dist = {
    'n_estimators': randint(50, 300),
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', 0.5, 0.8, 1.0]
}

# Randomized search
rf_random = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist,
    n_iter=50,  # Number of parameter combinations to try
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

rf_random.fit(X_train, y_train)

print("Best Parameters (Randomized):", rf_random.best_params_)
print("Best Score (Randomized):", rf_random.best_score_)
```

### Learning Curves for Model Validation

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

def plot_learning_curve(estimator, X, y, title="Learning Curve"):
    train_sizes, train_scores, val_scores = learning_curve(
        estimator, X, y, cv=5, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10)
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, 
                     train_mean + train_std, alpha=0.1)
    
    plt.plot(train_sizes, val_mean, 'o-', label='Validation Score')
    plt.fill_between(train_sizes, val_mean - val_std, 
                     val_mean + val_std, alpha=0.1)
    
    plt.xlabel('Training Set Size')
    plt.ylabel('Score')
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.show()

# Plot learning curves for different models
plot_learning_curve(RandomForestClassifier(n_estimators=100), 
                   X_train, y_train, "Random Forest Learning Curve")
```

## Practical Applications

### Real-World Use Cases

#### 1. **Medical Diagnosis**
- **Problem**: Diagnose diseases from patient data
- **Solution**: Random Forest for feature importance and robust predictions
- **Why RF**: Handles missing values, provides interpretable results

#### 2. **Financial Risk Assessment**
- **Problem**: Predict loan default risk
- **Solution**: Gradient Boosting for high accuracy
- **Why GB**: Sequential learning captures complex risk patterns

#### 3. **Image Classification**
- **Problem**: Classify images in computer vision
- **Solution**: Random Forest as baseline, then move to deep learning
- **Why RF**: Fast, robust, good starting point

#### 4. **Recommendation Systems**
- **Problem**: Recommend products to users
- **Solution**: Ensemble of different algorithms
- **Why Ensemble**: Combines collaborative and content-based filtering

### Best Practices

#### ✅ **Do's**
- Start with Random Forest as baseline
- Use cross-validation for model selection
- Monitor feature importance
- Consider computational constraints
- Validate on holdout test set

#### ❌ **Don'ts**
- Don't overfit to validation set
- Don't ignore feature scaling for some algorithms
- Don't use too many trees without validation
- Don't ignore class imbalance
- Don't forget to handle missing values appropriately

### Common Pitfalls and Solutions

| Problem | Cause | Solution |
|---------|------|----------|
| **Overfitting** | Too many trees, too deep | Reduce n_estimators, limit max_depth |
| **Slow Training** | Too many trees | Use fewer trees, parallel processing |
| **Poor Performance** | Wrong algorithm choice | Try different ensemble methods |
| **Memory Issues** | Large datasets | Use sampling, reduce features |
| **Unstable Results** | Random seed not set | Set random_state parameter |

## Hands-on Exercises

### Exercise 1: Basic Ensemble Comparison
```python
# Load the cancer dataset and compare different ensemble methods
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Load data
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# TODO: Implement and compare:
# 1. Single Decision Tree
# 2. Bagging
# 3. Random Forest
# 4. AdaBoost
# 5. Gradient Boosting

# Measure accuracy, precision, recall, and training time for each
```

### Exercise 2: Feature Importance Analysis
```python
# TODO: 
# 1. Train a Random Forest on the cancer dataset
# 2. Extract and visualize feature importance
# 3. Identify the top 10 most important features
# 4. Retrain model using only top 10 features
# 5. Compare performance with full feature set
```

### Exercise 3: Hyperparameter Tuning
```python
# TODO:
# 1. Use GridSearchCV to tune Random Forest parameters
# 2. Try different combinations of n_estimators, max_depth, min_samples_split
# 3. Use RandomizedSearchCV as alternative
# 4. Compare training time vs performance trade-offs
# 5. Plot learning curves for best model
```

### Exercise 4: Real Dataset Challenge
```python
# TODO: Apply ensemble methods to Housing.csv dataset
# 1. Load and preprocess the housing data
# 2. Handle missing values and categorical variables
# 3. Compare different ensemble methods for price prediction
# 4. Analyze feature importance
# 5. Create visualizations of results
```

### Exercise 5: Custom Ensemble
```python
# TODO: Create your own ensemble method
# 1. Combine Random Forest + Gradient Boosting
# 2. Use voting/averaging to combine predictions
# 3. Compare with individual methods
# 4. Analyze when your custom ensemble works best
```

## Summary

### Key Takeaways

1. **Ensemble Methods Work**: Combining multiple models usually improves performance
2. **Random Forest is King**: Often the best starting point for most problems
3. **Choose Wisely**: Different methods have different strengths and weaknesses
4. **Tune Carefully**: Hyperparameter tuning can significantly improve performance
5. **Validate Properly**: Always use cross-validation and holdout test sets

### Next Steps

- **Advanced Topics**: XGBoost, LightGBM, CatBoost
- **Deep Learning**: Neural networks and ensemble methods
- **Production**: Model deployment and monitoring
- **Research**: Stay updated with latest ensemble techniques

### Resources for Further Learning

- **Books**: "The Elements of Statistical Learning" by Hastie, Tibshirani, Friedman
- **Papers**: Original Random Forest paper by Breiman (2001)
- **Courses**: Machine Learning courses on Coursera, edX
- **Practice**: Kaggle competitions, real-world projects

---

*Remember: The best ensemble method depends on your specific problem, data, and constraints. Start with Random Forest, experiment with others, and always validate your results!*